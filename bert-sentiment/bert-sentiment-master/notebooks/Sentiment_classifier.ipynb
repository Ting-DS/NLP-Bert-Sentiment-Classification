{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4XNqCW7REKMK"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JJ2AenGe22H"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "qfP0GiNSdyiz",
    "outputId": "ab7820ac-c11c-49fa-8438-8e4a5c58e463"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cw8GRvneAeK"
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE_MODEL = 1e-5\n",
    "LEARNING_RATE_CLASSIFIER = 1e-3\n",
    "WARMUP_STEPS = 0\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "SEED = 42\n",
    "NO_CUDA = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPvYattqeC01"
   },
   "outputs": [],
   "source": [
    "def rpad(array, n):\n",
    "    current_len = len(array)\n",
    "    if current_len > n:\n",
    "        return array[:n]\n",
    "    extra = n - current_len\n",
    "    return array + ([0] * extra)\n",
    "\n",
    "\n",
    "def convert_to_embedding(tokenizer, sentences_with_labels):\n",
    "    for sentence, label in sentences_with_labels:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        tokens = tokens[:250]\n",
    "        bert_sent = rpad(tokenizer.convert_tokens_to_ids([\"CLS\"] + tokens + [\"SEP\"]), n=256)\n",
    "        yield torch.tensor(bert_sent), torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def parse_line(line):\n",
    "    line = line.strip().lower()\n",
    "    line = line.replace(\"&nbsp;\", \" \")\n",
    "    line = re.sub(r'<br(\\s\\/)?>', ' ', line)\n",
    "    line = re.sub(r' +', ' ', line)  # merge multiple spaces into one\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def read_imdb_data(filename):\n",
    "    data = []\n",
    "    for line in open(filename, 'r', encoding=\"utf-8\"):\n",
    "        data.append(parse_line(line))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_dataloader(tokenizer, sampler=RandomSampler, train=False):\n",
    "    filename = \"/content/gdrive/My Drive/bert/imdb_train.txt\" if train else \"/content/gdrive/My Drive/bert/imdb_test.txt\"\n",
    "    \n",
    "    data = read_imdb_data(filename)\n",
    "    y = np.append(np.zeros(12500), np.ones(12500))\n",
    "    sentences_with_labels = zip(data, y.tolist())\n",
    "\n",
    "    dataset = list(convert_to_embedding(tokenizer, sentences_with_labels))\n",
    "\n",
    "    sampler_func = sampler(dataset) if sampler is not None else None\n",
    "    dataloader = DataLoader(dataset, sampler=sampler_func, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thlcE_u4eHhs"
   },
   "outputs": [],
   "source": [
    "class Transformers:\n",
    "    model = None\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_token_label_id = PAD_TOKEN_LABEL_ID\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and not NO_CUDA else \"cpu\")\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            self.load()\n",
    "\n",
    "        embeddings = list(convert_to_embedding([(sentence, -1)]))\n",
    "        preds = self._predict_tags_batched(embeddings)\n",
    "        return preds\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        from sklearn.metrics import classification_report\n",
    "        y_pred = self._predict_tags_batched(dataloader)\n",
    "        y_true = np.append(np.zeros(12500), np.ones(12500))\n",
    "\n",
    "        score = classification_report(y_true, y_pred)\n",
    "        print(score)\n",
    "\n",
    "    def _predict_tags_batched(self, dataloader):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        for batch in tqdm(dataloader, desc=\"Computing NER tags\"):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch[0])\n",
    "                _, is_neg = torch.max(outputs[0], 1)\n",
    "                preds.extend(is_neg.cpu().detach().numpy())\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def train(self, dataloader, model, epochs):\n",
    "        assert self.model is None  # make sure we are not training after load() command\n",
    "        model.to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "        t_total = len(dataloader) // GRADIENT_ACCUMULATION_STEPS * epochs\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": model.bert.parameters(), \"lr\": LEARNING_RATE_MODEL},\n",
    "            {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=t_total)\n",
    "\n",
    "        # Train!\n",
    "        print(\"***** Running training *****\")\n",
    "        print(\"Training on %d examples\", len(dataloader))\n",
    "        print(\"Num Epochs = %d\", epochs)\n",
    "        print(\"Total optimization steps = %d\", t_total)\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        model.zero_grad()\n",
    "        train_iterator = trange(epochs, desc=\"Epoch\")\n",
    "        self._set_seed()\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(dataloader, desc=\"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                outputs = model(batch[0], labels=batch[1])\n",
    "                loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "                if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                    loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    optimizer.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        return global_step, tr_loss / global_step\n",
    "\n",
    "    def _set_seed(self):\n",
    "        torch.manual_seed(SEED)\n",
    "        if self.device == 'gpu':\n",
    "            torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    def load(self, model_dir='weights/'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "ZjoVtP7veLqz",
    "outputId": "2ffe19c3-5646-4016-c323-7a7d5d03987f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing NER tags: 100%|██████████| 1563/1563 [03:37<00:00,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.92      0.92     12500\n",
      "         1.0       0.92      0.92      0.92     12500\n",
      "\n",
      "    accuracy                           0.92     25000\n",
      "   macro avg       0.92      0.92      0.92     25000\n",
      "weighted avg       0.92      0.92      0.92     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epochs=20, output_dir=\"weights/\"):\n",
    "    num_labels = 2  # negative and positive reviews\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "    dataloader = prepare_dataloader(tokenizer, train=True)\n",
    "    predictor = Transformers(tokenizer)\n",
    "    predictor.train(dataloader, model, epochs)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "def evaluate(model_dir=\"weights/\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    dataloader = prepare_dataloader(tokenizer, train=False, sampler=None)\n",
    "    predictor = Transformers(tokenizer)\n",
    "    predictor.load(model_dir=model_dir)\n",
    "    predictor.evaluate(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "path = '/content/gdrive/My Drive/bert/weights/'\n",
    "#os.makedirs(path, exist_ok=True)\n",
    "#train(epochs=10, output_dir=path)\n",
    "evaluate(model_dir=path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
